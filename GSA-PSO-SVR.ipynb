{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import random\n",
    "from  sklearn.grid_search import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "import threading\n",
    "\n",
    "################################### 数据读取与预处理 ###############################################\n",
    "data = pd.read_excel('modeling_data.xlsx',sheetname='Area1_Load')\n",
    "features=pd.read_excel('modeling_data.xlsx',sheetname='Area1_Weather')\n",
    "## features前向填充缺失值\n",
    "features=features.fillna(method='ffill',axis=1)\n",
    "print(features.describe())\n",
    "## 数据标准化\n",
    "scaler_fea=StandardScaler()\n",
    "scaler_load=StandardScaler()\n",
    "## 选取历史负荷起始时间\n",
    "start=features.index[0]\n",
    "## 筛选具有特征的负荷\n",
    "data=data[data['YMD']>=start]\n",
    "## 删除预测填充空值部分\n",
    "data=data.dropna(how='any',axis=0)\n",
    "## 选取历史负荷终止时间\n",
    "end=data.iloc[-1,0]\n",
    "## 创建时间-负荷Dataframe\n",
    "load=pd.DataFrame()\n",
    "info=np.array(data.iloc[:,1:])\n",
    "## 求和获取日负荷值\n",
    "load_data=info.sum(axis=1)\n",
    "load_data=np.reshape(load_data,(-1,1))\n",
    "## 时间负荷Dataframe填充数据\n",
    "load['time']=data['YMD']\n",
    "load['daily_load']=load_data\n",
    "# 负荷趋势图\n",
    "# x_axis=[i for i in range(len(load['time']))]\n",
    "# plt.scatter(x_axis,load['daily_load'])\n",
    "# plt.show()\n",
    "\n",
    "######################################## 分析数据关系 ##############################################\n",
    "# trainset=features[features.index<=end].copy()\n",
    "# trainset['d_load']=load_data\n",
    "# trainset.columns=['year','month','week','H_tem','L_tem','Avg_tem','Hum','Pre','d_load']\n",
    "# year\n",
    "# 每一年的变化趋势\n",
    "# year2012=trainset.loc[trainset['year']==2012,['d_load']]\n",
    "# year2013=trainset.loc[trainset['year']==2013,['d_load']]\n",
    "# year2014=trainset.loc[trainset['year']==2014,['d_load']]\n",
    "# plt.subplot(311)\n",
    "# plt.plot([i for i in range(len(year2012))],year2012,'-*g',label='year 2012')\n",
    "# plt.subplot(312)\n",
    "# plt.plot([i for i in range(len(year2013))],year2013,'-*b',label='year 2013')\n",
    "# plt.subplot(313)\n",
    "# plt.plot([i for i in range(len(year2014))],year2014,'-*k',label='year 2014')\n",
    "# # 年均日负荷\n",
    "# year_avg_load=trainset[['year','d_load']].groupby('year').mean()\n",
    "# year_avg_load.plot.bar()\n",
    "# plt.title('year-avg_d_load')\n",
    "# plt.xlabel('year')\n",
    "# plt.ylabel('load')\n",
    "\n",
    "# month\n",
    "# year2012=trainset.loc[trainset['year']==2012,['d_load','month']]\n",
    "# for i in range(1,13):\n",
    "#     month=year2012.loc[year2012['month']==i,['d_load']]\n",
    "#     plt.figure()\n",
    "#     plt.plot([j for j in range(len(month))],month)\n",
    "#     plt.show()\n",
    "# # 月均日负荷\n",
    "# month_avg_load=trainset[['month','d_load','year']].groupby(['year','month']).mean()\n",
    "# month_avg_load.plot.bar()\n",
    "# plt.title('month-avg_d_load')\n",
    "# plt.xlabel('month')\n",
    "# plt.ylabel('load')\n",
    "\n",
    "# week\n",
    "# trainset[['week','d_load']].groupby('week').mean().plot.bar()\n",
    "\n",
    "# 平均温度(明显正相关)\n",
    "# trainset['Avg_tem_cut']=pd.qcut(trainset['Avg_tem'],3)\n",
    "# trainset['Avg_tem_cut']=pd.factorize(trainset['Avg_tem_cut'])[0]\n",
    "# trainset[['Avg_tem_cut','d_load']].groupby('Avg_tem_cut').mean().plot.bar()\n",
    "\n",
    "# 相对湿度（无太大影响）\n",
    "# trainset.describe()\n",
    "# trainset['Hum_cut']=pd.qcut(trainset['Hum'],6)\n",
    "# trainset['Hum_cut']=pd.factorize(trainset['Hum_cut'])[0]\n",
    "# trainset[['Hum_cut','d_load']].groupby('Hum_cut').mean().plot.bar()\n",
    "\n",
    "# 降水量(无太大影响)\n",
    "# print(trainset.describe())\n",
    "# pre_cut=[]\n",
    "# for pre in trainset['Pre']:\n",
    "#     if pre<=1:\n",
    "#         pre_cut.append(0)\n",
    "#     elif pre<=50:\n",
    "#         pre_cut.append(1)\n",
    "#     else:\n",
    "#         pre_cut.append(2)\n",
    "# trainset['Pre_cut']=pre_cut\n",
    "# trainset[['Pre_cut','d_load']].groupby('Pre_cut').mean().plot.bar()\n",
    "\n",
    "############################################# 特征工程与特征处理 ###########################################\n",
    "# 加入年份\\月份\\星期属性\n",
    "year=[];month=[];week=[];time=list(features.index)\n",
    "for i in range(len(time)):\n",
    "    t=str(time[i])\n",
    "    t_year=t[:4]\n",
    "    t_month=t[-4:-2]\n",
    "    ## 对星期属性进行处理\n",
    "    t_week=datetime.strptime(t,'%Y%m%d').weekday()+1\n",
    "    if t_week==6:\n",
    "        t_week=10\n",
    "    if t_week==7:\n",
    "        t_week=15\n",
    "    year.append(int(t_year))\n",
    "    month.append(int(t_month))\n",
    "    week.append(t_week)\n",
    "features.insert(0,'年份',year)\n",
    "features.insert(1,'月份',month)\n",
    "# 加入星期属性\n",
    "features.insert(2,'星期',week)\n",
    "features.columns=['year','month','week','H_tem','L_tem','Avg_tem','Hum','Pre']\n",
    "# 根据特征分析将降水量分为3类\n",
    "for i in range(len(features['Pre'])):\n",
    "    if features.iloc[i,-1]<=10:\n",
    "        features.iloc[i, -1]=0\n",
    "    elif features.iloc[i,-1]>=50:\n",
    "        features.iloc[i, -1] = 5\n",
    "    else:\n",
    "        features.iloc[i,-1]=2\n",
    "# 只对不包含（年份、月份\\星期类型和最后一列降水量）的数据标准化\n",
    "features=np.array(features)\n",
    "features[:,3:-1]=scaler_fea.fit_transform(features[:,3:-1])\n",
    "all_load=np.reshape(np.array(load['daily_load']),(-1,1))\n",
    "all_load=scaler_load.fit_transform(all_load)\n",
    "# 选取预测输入并标准化\n",
    "# features[:,2:-1]=(features[:,2:-1]-scaler_fea.mean_)/np.sqrt(scaler_fea.var_)\n",
    "# 选取历史与预测的输入输出\n",
    "pre_day=10\n",
    "begin=len(load_data)-pre_day\n",
    "over=len(load_data)\n",
    "his_x=features[:begin,:]\n",
    "his_load=all_load[:begin,:]\n",
    "print(his_x.shape)\n",
    "pre_x=features[begin:over,:]\n",
    "# pre_load=np.zeros((pre_x.shape[0],1))\n",
    "actual_load=load_data[-pre_day:,:]\n",
    "print(pre_x.shape)\n",
    "# 数据预处理后保存至本地csv文件\n",
    "csv=np.concatenate((features[:over,:],load_data),axis=1)\n",
    "csv=pd.DataFrame(csv)\n",
    "csv.columns=['year','month','week','H_tem','L_tem','Avg_tem','Hum','Pre','Daily_load']\n",
    "csv.to_csv('modeling_load.csv',index=None)\n",
    "\n",
    "##################################### 特征属性可视化分析 #######################################\n",
    "# 互信息权重分析\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "model=SelectKBest(mutual_info_regression,k=8) # 删除互信息最小的因子\n",
    "#输出互信息的大小\n",
    "his_x=model.fit_transform(his_x,his_load)\n",
    "weight=list(model.scores_)\n",
    "\n",
    "# pearson相关系数分析\n",
    "import seaborn as sns\n",
    "correlation=csv.copy()\n",
    "correlation=correlation.astype(float)\n",
    "corr=correlation.corr()\n",
    "# sns.heatmap(corr,annot=True)\n",
    "\n",
    "\n",
    "############################################ 模型融合与测试 ########################################\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "feature_names=corr.columns[:-1]\n",
    "print(type(feature_names))\n",
    "# RFR\n",
    "rf_param={'n_estimators':[50,60,70,80,90,100],'min_samples_split':[3,4,5],'min_samples_leaf':[2,3,4],'max_depth':[4]}\n",
    "rf=RandomForestRegressor()\n",
    "rf_grid=GridSearchCV(rf,param_grid=rf_param,scoring='neg_mean_squared_error')\n",
    "rf_grid.fit(his_x,his_load)\n",
    "rf_importance=rf_grid.best_estimator_.feature_importances_\n",
    "rf_importance=list(sorted(zip(rf_importance,feature_names),reverse=True))[:-1]\n",
    "rf_feature=[i[1] for i in rf_importance]\n",
    "rf_best_param=rf_grid.best_params_\n",
    "\n",
    "# ADB\n",
    "adb_param={'n_estimators':[i for i in range(80,200,10)],'learning_rate':list(np.linspace(0.01,1,10))}\n",
    "adb=AdaBoostRegressor(loss='square')\n",
    "adb_grid=GridSearchCV(adb,param_grid=adb_param,scoring='neg_mean_squared_error')\n",
    "adb_grid.fit(his_x,his_load)\n",
    "adb_importance=adb_grid.best_estimator_.feature_importances_\n",
    "adb_importance=list(sorted(zip(adb_importance,feature_names),reverse=True))[:-1]\n",
    "adb_feature=[i[1] for i in adb_importance]\n",
    "adb_best_param=adb_grid.best_params_\n",
    "\n",
    "# GBR\n",
    "gbr=GradientBoostingRegressor(loss='ls')\n",
    "gbr_param={'n_estimators':[i for i in range(100,200,10)],'max_features':[3,4,5,6],\n",
    "           'learning_rate':list(np.linspace(0.01,1,10)),'min_samples_split':[2,3,4,5],\n",
    "           'min_samples_leaf':[2,3,4,5]}\n",
    "gbr_grid=GridSearchCV(gbr,param_grid=gbr_param,scoring='neg_mean_squared_error')\n",
    "gbr_grid.fit(his_x,his_load)\n",
    "gbr_importance=gbr_grid.best_estimator_.feature_importances_\n",
    "gbr_importance=list(sorted(zip(gbr_importance,feature_names),reverse=True))[:-1]\n",
    "gbr_feature=[i[1] for i in gbr_importance]\n",
    "gbr_best_param=gbr_grid.best_params_\n",
    "\n",
    "# 特征融合与选择、\n",
    "final_features=list(set(rf_feature+adb_feature+gbr_feature))\n",
    "his_x,pre_x=pd.DataFrame(his_x),pd.DataFrame(pre_x)\n",
    "his_x.columns=['year','month','week','H_tem','L_tem','Avg_tem','Hum','Pre']\n",
    "pre_x.columns=['year','month','week','H_tem','L_tem','Avg_tem','Hum','Pre']\n",
    "out_features=[item for item in list(his_x.columns) if item not in final_features]\n",
    "del his_x[out_features[0]]; del pre_x[out_features[0]]\n",
    "his_x,pre_x=np.array(his_x),np.array(pre_x)\n",
    "\n",
    "\n",
    "\n",
    "############################################ GSA-CV-SVR #############################################\n",
    "svr=SVR(kernel='rbf')\n",
    "C_value=list(np.linspace(0.1,10,20))\n",
    "gamma_value=list(np.linspace(0.1,5,20))\n",
    "svr_param={'C':C_value,'gamma':gamma_value}\n",
    "svr_grid=GridSearchCV(svr,param_grid=svr_param,cv=10)\n",
    "svr_grid.fit(his_x,his_load)\n",
    "# 属性输出\n",
    "grid_scores=[score[1] for score in svr_grid.grid_scores_]\n",
    "best_param=svr_grid.best_params_\n",
    "best_score=svr_grid.best_score_\n",
    "print(best_param,best_score)\n",
    "best_C=best_param['C']\n",
    "best_gamma=best_param['gamma']\n",
    "# 得到全局最佳参数的回归器\n",
    "# Advanced_svr=SVR(kernel='rbf',C=best_C,gamma=best_gamma)\n",
    "# Advanced_svr.fit(his_x,his_load)\n",
    "# Advanced_svr_load=Advanced_svr.predict(pre_x)\n",
    "# Advanced_svr_load=scaler_load.inverse_transform(Advanced_svr_load)\n",
    "# print(Advanced_svr_load,actual_load)\n",
    "# Advanced_svr_load=np.reshape(Advanced_svr_load,(-1,1))\n",
    "# pre_result=np.concatenate((Advanced_svr_load,actual_load),axis=1)\n",
    "\n",
    "## 等高线图确定全局最优范围\n",
    "def contourmap(C,gamma,grid_scores):\n",
    "    x=np.array(C)\n",
    "    y=np.array(gamma)\n",
    "    gs=[k for k in grid_scores]\n",
    "    scores=[];score=[]\n",
    "    for index, value in enumerate(gs):\n",
    "        if index > 0 and index % (len(x)) == 0:\n",
    "            scores.append(score)\n",
    "            score=[]\n",
    "        score.append(value)\n",
    "        if index==(len(gs)-1):\n",
    "            scores.append(score)\n",
    "    X,Y=np.meshgrid(x,y)\n",
    "    plt.contourf(X,Y,scores,10,alpha=0.5,cmap=plt.cm.hot)\n",
    "    C=plt.contour(X,Y,scores,10,colors='blue',linewidth=0.5)\n",
    "    plt.clabel(C,fontsize=20)\n",
    "    plt.xlabel('C')\n",
    "    plt.ylabel('gamma')\n",
    "    plt.title('The contour map of SVR parameters')\n",
    "    plt.show()\n",
    "# 绘制等高线图\n",
    "# contourmap(C_value,gamma_value,grid_scores)\n",
    "\n",
    "# PSO参数优化\n",
    "class PSO(object):\n",
    "    def __init__(self, population_size, max_steps):\n",
    "        self.w=0.6  # 惯性权重\n",
    "        self.c1=self.c2 = 2\n",
    "        self.population_size=population_size  # 粒子群数量\n",
    "        self.dim=2  # 搜索空间的维度\n",
    "        self.max_steps=max_steps  # 迭代次数\n",
    "        self.x_bound=[0.1,5]  # 解空间范围\n",
    "        self.x=np.random.uniform(self.x_bound[0], self.x_bound[1],\n",
    "                                (self.population_size, self.dim))  # 初始化粒子群位置\n",
    "        self.v=np.random.rand(self.population_size, self.dim)  # 初始化粒子群速度\n",
    "        fitness,best_param=self.calculate_fitness(self.x)\n",
    "        self.p=self.x # 个体的最佳位置\n",
    "        self.param=best_param # 记录个体最佳参数\n",
    "        self.individual_best_fitness=fitness  # 个体的最优适应度\n",
    "        self.fitness_score=[]\n",
    "    ###定义目标损失函数\n",
    "    def calculate_fitness(self,x):\n",
    "        param_grid={'C':list(abs(x[:,0])),'gamma':list(abs(x[:,1]))}\n",
    "        svr=GridSearchCV(SVR(kernel='rbf'),param_grid,cv=5)\n",
    "        svr.fit(his_x,his_load)\n",
    "        grid_scores=svr.best_score_\n",
    "        best_param=svr.best_params_\n",
    "        return grid_scores,best_param\n",
    "\n",
    "    def evolve(self):\n",
    "        fig = plt.figure()\n",
    "        for step in range(self.max_steps):\n",
    "            r1=np.random.rand(self.population_size,self.dim)\n",
    "            r2=np.random.rand(self.population_size,self.dim)\n",
    "            # 更新速度和权重\n",
    "            self.v=self.w*self.v+self.c1*r1*(self.p-self.x)\n",
    "            self.x=self.v+self.x\n",
    "            # 粒子变化散点图\n",
    "            plt.clf()\n",
    "            plt.scatter(self.x[:, 0],self.x[:, 1],s=30,color='k')\n",
    "            plt.xlabel('Parameter C')\n",
    "            plt.ylabel('Parameter gamma')\n",
    "            plt.title('Particle motion scatter plot')\n",
    "            plt.xlim(self.x_bound[0],self.x_bound[1])\n",
    "            plt.ylim(self.x_bound[0],self.x_bound[1])\n",
    "            plt.show()\n",
    "            plt.pause(0.01)\n",
    "            fitness,best_param=self.calculate_fitness(self.x)\n",
    "            # 需要更新的个体\n",
    "            if fitness<self.individual_best_fitness:\n",
    "                self.p=self.x\n",
    "                self.param=best_param\n",
    "                self.individual_best_fitness=fitness\n",
    "            self.fitness_score.append(self.individual_best_fitness)\n",
    "        # 绘制迭代次数与适应度误差相关曲线\n",
    "        x_axis=[i for i in range(self.max_steps)]\n",
    "        plt.figure(figsize=(15,9))\n",
    "        plt.plot(x_axis,self.fitness_score,'-^b')\n",
    "        plt.xlabel('Iterative number')\n",
    "        plt.ylabel('Fitness value')\n",
    "        plt.title('Iterative error plot')\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "        return self.param,self.individual_best_fitness\n",
    "# 对象实现局部搜索\n",
    "# def main():\n",
    "# 等高线图粗略选择\n",
    "contourmap(C_value, gamma_value, grid_scores)\n",
    "# PSO参数选取\n",
    "my_pso=PSO(20,15)\n",
    "local_best_param,best_score=my_pso.evolve()\n",
    "local_best_C,local_best_gamma=local_best_param['C'],local_best_param['gamma']\n",
    "# 得到局部最佳参数的回归器\n",
    "Advanced_svr=SVR(kernel='rbf',C=local_best_C,gamma=local_best_gamma)\n",
    "Advanced_svr.fit(his_x,his_load)\n",
    "Advanced_svr_load=Advanced_svr.predict(pre_x)\n",
    "Advanced_svr_load=scaler_load.inverse_transform(Advanced_svr_load)\n",
    "Advanced_svr_load=np.reshape(Advanced_svr_load,(-1,1))\n",
    "pre_result=np.concatenate((Advanced_svr_load,actual_load),axis=1)\n",
    "print(pre_result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
